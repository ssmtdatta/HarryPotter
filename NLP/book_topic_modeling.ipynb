{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds key topics from all documents extracted from the books. Documents are text segments. Segment size is defined by users.\n",
    "Using topic modeling techniques such as LDA, LSI, key topics are extracted from the documents.\n",
    "\n",
    "Text segments/documents are clustered (using k-means) into topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"~/HarryPotter/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(path.abspath(BASE_DIR+'NLP/SupportModules/'))\n",
    "import data_transform_module as dtmod\n",
    "import book_module as bmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BOOK_PATH = BASE_DIR+'NLP/BookData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling with Gensim LDA\n",
    "* generate term-matrix for LDA (sklearn --> countVectorizer --> term-matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports for LDA\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():  \n",
    "    \"\"\"\n",
    "    input is one book as dictionary.\n",
    "    each list in the dictionary is a chapter.\n",
    "    \"\"\"\n",
    "    def __init__(self, book, chunk_size):\n",
    "        self.book = book\n",
    "        self.chunk_size = chunk_size\n",
    "        self.docs = self.get_bookDocuments()\n",
    "    \n",
    "    def get_bookDocuments(self):\n",
    "        book_documents = []\n",
    "        for key in self.book:\n",
    "            chapter = self.book[key].lower()\n",
    "            chapter_tokens = word_tokenize(chapter)         \n",
    "            \n",
    "            if len(chapter_tokens) >= self.chunk_size: \n",
    "                start_ind = list(range(0,len(chapter_tokens), self.chunk_size))\n",
    "                end_ind = start_ind[1:] + [len(chapter_tokens)]\n",
    "                \n",
    "                for ind in range(0, len(start_ind)):\n",
    "                    tokenized_chunk = chapter_tokens[start_ind[ind]:end_ind[ind]]\n",
    "                    chunk = ' '.join(x for x in tokenized_chunk)\n",
    "                    book_documents.append(chunk)\n",
    "        return(book_documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_LDA_topicModel(all_documents):    \n",
    "    # Create a CountVectorizer \n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2),  stop_words='english')\n",
    "    # Fit it on the document data (trining?)\n",
    "    count_vectorizer.fit(all_documents)\n",
    "    # Create the term-document matrix (Transpose places terms on the rows)\n",
    "    counts = count_vectorizer.transform(all_documents).transpose()\n",
    "    # Convert sparse matrix of counts to a gensim corpus\n",
    "    corpus = matutils.Sparse2Corpus(counts)\n",
    "    # Map matrix rows to words (tokens)\n",
    "    id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())\n",
    "    \n",
    "    # Create lda model\n",
    "    lda = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=10)\n",
    "    #lda topics\n",
    "    lda_print_topics = lda.print_topics()\n",
    "    # Transform the docs from the word space to the topic space\n",
    "    lda_corpus = lda[corpus]\n",
    "    \n",
    "    return lda_corpus, lda_print_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2902 obtained from all 7 books with a chunk size of 400\n"
     ]
    }
   ],
   "source": [
    "all_documents = []\n",
    "chunk_size = 400\n",
    "for seq in range(1, 8):\n",
    "    book_dict = dtmod.unpickleSomething(BOOK_PATH, \"book{}_chapters.p\".format(seq))\n",
    "    doc = Document(book_dict, chunk_size)\n",
    "    book_documents = doc.docs\n",
    "    all_documents = all_documents + book_documents\n",
    "\n",
    "print(len(all_documents), \"obtained from all 7 books with a chunk size of\", chunk_size )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 38s, sys: 22.9 s, total: 8min 1s\n",
      "Wall time: 8min 3s\n"
     ]
    }
   ],
   "source": [
    "%time lda_corpus_output, lda_print_topics  = get_LDA_topicModel(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(45,\n",
       "  '0.001*\"dumbledore\" + 0.001*\"morfin\" + 0.001*\"belby\" + 0.001*\"horcruxes\" + 0.001*\"fleur\" + 0.001*\"mclaggen\" + 0.001*\"known\" + 0.001*\"ze\" + 0.001*\"scoop\" + 0.001*\"meant\"'),\n",
       " (8,\n",
       "  '0.004*\"dirk\" + 0.001*\"cresswell\" + 0.001*\"dirk cresswell\" + 0.001*\"crackled\" + 0.001*\"rons\" + 0.001*\"dean\" + 0.001*\"thought hear\" + 0.001*\"going don\" + 0.001*\"lifted air\" + 0.001*\"irksome\"'),\n",
       " (12,\n",
       "  '0.008*\"said\" + 0.007*\"weasley\" + 0.006*\"harry\" + 0.004*\"mrs\" + 0.004*\"mrs weasley\" + 0.003*\"ginny\" + 0.003*\"mr\" + 0.002*\"mr weasley\" + 0.002*\"fred\" + 0.002*\"george\"'),\n",
       " (44,\n",
       "  '0.005*\"bellatrix\" + 0.004*\"draco\" + 0.003*\"lord\" + 0.003*\"greyback\" + 0.003*\"lucius\" + 0.003*\"ted\" + 0.003*\"narcissa\" + 0.002*\"death\" + 0.002*\"voldemort\" + 0.002*\"malfoy\"'),\n",
       " (29,\n",
       "  '0.005*\"professor\" + 0.001*\"professor mcgonagall\" + 0.001*\"nearly headless\" + 0.001*\"mcgonagall\" + 0.001*\"said professor\" + 0.001*\"headless\" + 0.001*\"nick\" + 0.001*\"headless nick\" + 0.001*\"trelawney\" + 0.001*\"professor umbridge\"'),\n",
       " (48,\n",
       "  '0.022*\"vernon\" + 0.021*\"uncle\" + 0.019*\"uncle vernon\" + 0.017*\"dudley\" + 0.012*\"aunt\" + 0.012*\"petunia\" + 0.009*\"aunt petunia\" + 0.005*\"dursleys\" + 0.003*\"harry\" + 0.003*\"said uncle\"'),\n",
       " (32,\n",
       "  '0.005*\"harry\" + 0.003*\"professor slughorn\" + 0.002*\"dumbledore\" + 0.001*\"slughorn\" + 0.001*\"eyes\" + 0.001*\"said\" + 0.001*\"mr\" + 0.001*\"black\" + 0.001*\"snape\" + 0.001*\"island\"'),\n",
       " (6,\n",
       "  '0.001*\"dudley\" + 0.001*\"sensor\" + 0.001*\"raised wand\" + 0.001*\"moaning\" + 0.001*\"harry\" + 0.001*\"secrecy sensor\" + 0.001*\"hi luna\" + 0.001*\"hole harry\" + 0.001*\"voldemort\" + 0.001*\"voldemort harry\"'),\n",
       " (39,\n",
       "  '0.034*\"harry\" + 0.024*\"said\" + 0.013*\"ron\" + 0.012*\"hermione\" + 0.006*\"did\" + 0.005*\"said harry\" + 0.004*\"looked\" + 0.004*\"know\" + 0.004*\"like\" + 0.004*\"just\"'),\n",
       " (0,\n",
       "  '0.002*\"gryffindor\" + 0.002*\"bludger\" + 0.002*\"snitch\" + 0.002*\"team\" + 0.002*\"harper\" + 0.001*\"hooch\" + 0.001*\"madam hooch\" + 0.001*\"katie\" + 0.001*\"wood\" + 0.001*\"coote\"'),\n",
       " (35,\n",
       "  '0.001*\"nick\" + 0.001*\"halves\" + 0.001*\"baron\" + 0.001*\"looking bewildered\" + 0.001*\"peeves\" + 0.001*\"headless nick\" + 0.001*\"nearly headless\" + 0.001*\"necklace\" + 0.001*\"ter\" + 0.001*\"headless\"'),\n",
       " (21,\n",
       "  '0.004*\"mr\" + 0.002*\"weasley\" + 0.002*\"crouch\" + 0.001*\"said mr\" + 0.001*\"mr crouch\" + 0.001*\"mr weasley\" + 0.001*\"scrimgeour\" + 0.001*\"diggory\" + 0.001*\"mr diggory\" + 0.001*\"percy\"'),\n",
       " (34,\n",
       "  '0.006*\"locket\" + 0.006*\"snape\" + 0.004*\"harry\" + 0.003*\"professor\" + 0.002*\"lupin\" + 0.001*\"said snape\" + 0.001*\"dark lord\" + 0.001*\"mcgonagall\" + 0.001*\"potter\" + 0.001*\"professor mcgonagall\"'),\n",
       " (3,\n",
       "  '0.011*\"hagrid\" + 0.005*\"malfoy\" + 0.004*\"yeh\" + 0.003*\"crabbe\" + 0.003*\"goyle\" + 0.003*\"aragog\" + 0.002*\"said hagrid\" + 0.002*\"ter\" + 0.002*\"crabbe goyle\" + 0.001*\"forest\"'),\n",
       " (20,\n",
       "  '0.023*\"said\" + 0.019*\"harry\" + 0.012*\"dumbledore\" + 0.007*\"ron\" + 0.006*\"did\" + 0.005*\"know\" + 0.005*\"hermione\" + 0.003*\"think\" + 0.003*\"lupin\" + 0.003*\"said harry\"'),\n",
       " (16,\n",
       "  '0.003*\"kreacher\" + 0.002*\"snape\" + 0.001*\"ted\" + 0.001*\"dark lord\" + 0.001*\"greyback\" + 0.001*\"regulus\" + 0.001*\"lord\" + 0.001*\"love said\" + 0.001*\"said ted\" + 0.001*\"master\"'),\n",
       " (47,\n",
       "  '0.001*\"dance\" + 0.001*\"copy\" + 0.001*\"place hide\" + 0.001*\"asked luna\" + 0.001*\"dance floor\" + 0.001*\"advanced potion\" + 0.001*\"copy advanced\" + 0.001*\"book\" + 0.001*\"hide\" + 0.001*\"hide book\"'),\n",
       " (14,\n",
       "  '0.003*\"dobby\" + 0.003*\"potter\" + 0.002*\"harry potter\" + 0.002*\"borgin\" + 0.002*\"sir\" + 0.002*\"malfoy\" + 0.001*\"mr\" + 0.001*\"umbridge\" + 0.001*\"elf\" + 0.001*\"professor umbridge\"'),\n",
       " (43,\n",
       "  '0.007*\"aberforth\" + 0.003*\"said aberforth\" + 0.001*\"brother\" + 0.001*\"charlie\" + 0.001*\"ariana\" + 0.001*\"dumbledore body\" + 0.001*\"said charlie\" + 0.001*\"greed\" + 0.001*\"norbert\" + 0.001*\"hedge\"'),\n",
       " (22,\n",
       "  '0.008*\"harry\" + 0.006*\"said\" + 0.005*\"professor\" + 0.002*\"riddle\" + 0.002*\"mcgonagall\" + 0.002*\"professor mcgonagall\" + 0.002*\"did\" + 0.002*\"said professor\" + 0.002*\"sir\" + 0.002*\"black\"')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_print_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2902"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_docs = [doc for doc in lda_corpus_output]\n",
    "len(lda_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lda_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(45,\n",
       "  '0.001*\"dumbledore\" + 0.001*\"morfin\" + 0.001*\"belby\" + 0.001*\"horcruxes\" + 0.001*\"fleur\" + 0.001*\"mclaggen\" + 0.001*\"known\" + 0.001*\"ze\" + 0.001*\"scoop\" + 0.001*\"meant\"'),\n",
       " (8,\n",
       "  '0.004*\"dirk\" + 0.001*\"cresswell\" + 0.001*\"dirk cresswell\" + 0.001*\"crackled\" + 0.001*\"rons\" + 0.001*\"dean\" + 0.001*\"thought hear\" + 0.001*\"going don\" + 0.001*\"lifted air\" + 0.001*\"irksome\"'),\n",
       " (12,\n",
       "  '0.008*\"said\" + 0.007*\"weasley\" + 0.006*\"harry\" + 0.004*\"mrs\" + 0.004*\"mrs weasley\" + 0.003*\"ginny\" + 0.003*\"mr\" + 0.002*\"mr weasley\" + 0.002*\"fred\" + 0.002*\"george\"'),\n",
       " (44,\n",
       "  '0.005*\"bellatrix\" + 0.004*\"draco\" + 0.003*\"lord\" + 0.003*\"greyback\" + 0.003*\"lucius\" + 0.003*\"ted\" + 0.003*\"narcissa\" + 0.002*\"death\" + 0.002*\"voldemort\" + 0.002*\"malfoy\"'),\n",
       " (29,\n",
       "  '0.005*\"professor\" + 0.001*\"professor mcgonagall\" + 0.001*\"nearly headless\" + 0.001*\"mcgonagall\" + 0.001*\"said professor\" + 0.001*\"headless\" + 0.001*\"nick\" + 0.001*\"headless nick\" + 0.001*\"trelawney\" + 0.001*\"professor umbridge\"'),\n",
       " (48,\n",
       "  '0.022*\"vernon\" + 0.021*\"uncle\" + 0.019*\"uncle vernon\" + 0.017*\"dudley\" + 0.012*\"aunt\" + 0.012*\"petunia\" + 0.009*\"aunt petunia\" + 0.005*\"dursleys\" + 0.003*\"harry\" + 0.003*\"said uncle\"'),\n",
       " (32,\n",
       "  '0.005*\"harry\" + 0.003*\"professor slughorn\" + 0.002*\"dumbledore\" + 0.001*\"slughorn\" + 0.001*\"eyes\" + 0.001*\"said\" + 0.001*\"mr\" + 0.001*\"black\" + 0.001*\"snape\" + 0.001*\"island\"'),\n",
       " (6,\n",
       "  '0.001*\"dudley\" + 0.001*\"sensor\" + 0.001*\"raised wand\" + 0.001*\"moaning\" + 0.001*\"harry\" + 0.001*\"secrecy sensor\" + 0.001*\"hi luna\" + 0.001*\"hole harry\" + 0.001*\"voldemort\" + 0.001*\"voldemort harry\"'),\n",
       " (39,\n",
       "  '0.034*\"harry\" + 0.024*\"said\" + 0.013*\"ron\" + 0.012*\"hermione\" + 0.006*\"did\" + 0.005*\"said harry\" + 0.004*\"looked\" + 0.004*\"know\" + 0.004*\"like\" + 0.004*\"just\"'),\n",
       " (0,\n",
       "  '0.002*\"gryffindor\" + 0.002*\"bludger\" + 0.002*\"snitch\" + 0.002*\"team\" + 0.002*\"harper\" + 0.001*\"hooch\" + 0.001*\"madam hooch\" + 0.001*\"katie\" + 0.001*\"wood\" + 0.001*\"coote\"'),\n",
       " (35,\n",
       "  '0.001*\"nick\" + 0.001*\"halves\" + 0.001*\"baron\" + 0.001*\"looking bewildered\" + 0.001*\"peeves\" + 0.001*\"headless nick\" + 0.001*\"nearly headless\" + 0.001*\"necklace\" + 0.001*\"ter\" + 0.001*\"headless\"'),\n",
       " (21,\n",
       "  '0.004*\"mr\" + 0.002*\"weasley\" + 0.002*\"crouch\" + 0.001*\"said mr\" + 0.001*\"mr crouch\" + 0.001*\"mr weasley\" + 0.001*\"scrimgeour\" + 0.001*\"diggory\" + 0.001*\"mr diggory\" + 0.001*\"percy\"'),\n",
       " (34,\n",
       "  '0.006*\"locket\" + 0.006*\"snape\" + 0.004*\"harry\" + 0.003*\"professor\" + 0.002*\"lupin\" + 0.001*\"said snape\" + 0.001*\"dark lord\" + 0.001*\"mcgonagall\" + 0.001*\"potter\" + 0.001*\"professor mcgonagall\"'),\n",
       " (3,\n",
       "  '0.011*\"hagrid\" + 0.005*\"malfoy\" + 0.004*\"yeh\" + 0.003*\"crabbe\" + 0.003*\"goyle\" + 0.003*\"aragog\" + 0.002*\"said hagrid\" + 0.002*\"ter\" + 0.002*\"crabbe goyle\" + 0.001*\"forest\"'),\n",
       " (20,\n",
       "  '0.023*\"said\" + 0.019*\"harry\" + 0.012*\"dumbledore\" + 0.007*\"ron\" + 0.006*\"did\" + 0.005*\"know\" + 0.005*\"hermione\" + 0.003*\"think\" + 0.003*\"lupin\" + 0.003*\"said harry\"'),\n",
       " (16,\n",
       "  '0.003*\"kreacher\" + 0.002*\"snape\" + 0.001*\"ted\" + 0.001*\"dark lord\" + 0.001*\"greyback\" + 0.001*\"regulus\" + 0.001*\"lord\" + 0.001*\"love said\" + 0.001*\"said ted\" + 0.001*\"master\"'),\n",
       " (47,\n",
       "  '0.001*\"dance\" + 0.001*\"copy\" + 0.001*\"place hide\" + 0.001*\"asked luna\" + 0.001*\"dance floor\" + 0.001*\"advanced potion\" + 0.001*\"copy advanced\" + 0.001*\"book\" + 0.001*\"hide\" + 0.001*\"hide book\"'),\n",
       " (14,\n",
       "  '0.003*\"dobby\" + 0.003*\"potter\" + 0.002*\"harry potter\" + 0.002*\"borgin\" + 0.002*\"sir\" + 0.002*\"malfoy\" + 0.001*\"mr\" + 0.001*\"umbridge\" + 0.001*\"elf\" + 0.001*\"professor umbridge\"'),\n",
       " (43,\n",
       "  '0.007*\"aberforth\" + 0.003*\"said aberforth\" + 0.001*\"brother\" + 0.001*\"charlie\" + 0.001*\"ariana\" + 0.001*\"dumbledore body\" + 0.001*\"said charlie\" + 0.001*\"greed\" + 0.001*\"norbert\" + 0.001*\"hedge\"'),\n",
       " (22,\n",
       "  '0.008*\"harry\" + 0.006*\"said\" + 0.005*\"professor\" + 0.002*\"riddle\" + 0.002*\"mcgonagall\" + 0.002*\"professor mcgonagall\" + 0.002*\"did\" + 0.002*\"said professor\" + 0.002*\"sir\" + 0.002*\"black\"')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_print_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling with Gensim LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():  \n",
    "    \"\"\"\n",
    "    input is one book as dictionary.\n",
    "    each list in the dictionary is a chapter.\n",
    "    \"\"\"\n",
    "    def __init__(self, book, chunk_size):\n",
    "        self.book = book\n",
    "        self.chunk_size = chunk_size\n",
    "        self.docs = self.get_bookDocuments()\n",
    "    \n",
    "    def get_bookDocuments(self):\n",
    "        book_documents = []\n",
    "        for key in self.book:\n",
    "            chapter = self.book[key].lower()\n",
    "            chapter_tokens = word_tokenize(chapter)         \n",
    "            \n",
    "            if len(chapter_tokens) >= self.chunk_size: \n",
    "                start_ind = list(range(0,len(chapter_tokens), self.chunk_size))\n",
    "                end_ind = start_ind[1:] + [len(chapter_tokens)]\n",
    "                \n",
    "                for ind in range(0, len(start_ind)):\n",
    "                    tokenized_chunk = chapter_tokens[start_ind[ind]:end_ind[ind]]\n",
    "                    chunk = ' '.join(x for x in tokenized_chunk)\n",
    "                    book_documents.append(chunk)\n",
    "        return(book_documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_LSI_topicModel(all_documents):\n",
    "    \n",
    "    # Create a CountVectorizer \n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2),  stop_words='english')\n",
    "    # Fit it on the document data (trining?)\n",
    "    count_vectorizer.fit(all_documents)\n",
    "    \n",
    "    # Create the term-document matrix (Transpose places terms on the rows)\n",
    "    counts = count_vectorizer.transform(all_documents).transpose()\n",
    "    print(\"shape of counts\", counts.shape)\n",
    "    # Convert sparse matrix of counts to a gensim corpus\n",
    "    corpus = matutils.Sparse2Corpus(counts)\n",
    "    # Map matrix rows to words (tokens)\n",
    "    id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())\n",
    "    \n",
    "    # Create lda model\n",
    "    lsi = models.LsiModel(corpus=corpus, id2word=id2word, num_topics=50)\n",
    "    #lda topics\n",
    "    lsi_print_topics = lsi.print_topics()\n",
    "    length = len(lsi_print_topics)\n",
    "    # Transform the docs from the word space to the topic space\n",
    "    lsi_corpus = lsi[corpus]\n",
    "    \n",
    "    return lsi_corpus, lsi_print_topics, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2902 obtained from all 7 books with a chunk size of 400\n"
     ]
    }
   ],
   "source": [
    "all_documents = []\n",
    "chunk_size = 400\n",
    "for seq in range(1, 8):\n",
    "    book_dict = dtmod.unpickleSomething(BOOK_PATH, \"book{}_chapters.p\".format(seq))\n",
    "    doc = Document(book_dict, chunk_size)\n",
    "    book_documents = doc.docs\n",
    "    all_documents = all_documents + book_documents\n",
    "\n",
    "print(len(all_documents), \"obtained from all 7 books with a chunk size of\", chunk_size )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of counts (354132, 2902)\n",
      "CPU times: user 58.2 s, sys: 2.86 s, total: 1min 1s\n",
      "Wall time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "%time lsi_corpus_output, lsi_print_topics, length  = get_LSI_topicModel(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 354132)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length, len(lsi_corpus_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lsi_docs = [doc for doc in lsi_corpus_output]\n",
    "#lsi_docs are coordinates in topic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.042*\"accident send\" + 0.040*\"able timid\" + 0.039*\"accidents don\" + 0.039*\"accountant talk\" + 0.039*\"address harry\" + 0.038*\"achingly\" + 0.037*\"abuse said\" + 0.037*\"added drops\" + 0.036*\"actually attempting\" + 0.036*\"added voldemort\"'),\n",
       " (1,\n",
       "  '-0.092*\"accepts harry\" + -0.077*\"abysmal\" + -0.076*\"ache wanted\" + -0.066*\"abuse said\" + -0.065*\"accepting voldemort\" + -0.060*\"able blast\" + -0.060*\"abstinence precisely\" + 0.059*\"aberforth glared\" + -0.057*\"addressed silent\" + -0.057*\"accordance educational\"'),\n",
       " (2,\n",
       "  '0.078*\"actively prevent\" + 0.072*\"account terrible\" + 0.071*\"accio locker\" + 0.071*\"accompanied certain\" + 0.068*\"abou clamberin\" + 0.067*\"admit hagrid\" + -0.063*\"advice ron\" + -0.062*\"advice sending\" + 0.061*\"accio horcrux\" + -0.060*\"added voicing\"'),\n",
       " (3,\n",
       "  '0.127*\"absentmindedly touched\" + 0.100*\"absently dug\" + 0.099*\"absently new\" + 0.093*\"accurate title\" + 0.092*\"advancing ron\" + 0.089*\"absorbed going\" + 0.088*\"absentmindedly large\" + 0.086*\"abstained\" + 0.086*\"added definite\" + 0.085*\"added defiantly\"'),\n",
       " (4,\n",
       "  '-0.110*\"actually course\" + -0.107*\"actually dunno\" + -0.105*\"act daughter\" + -0.104*\"admit\" + -0.100*\"aaargh yelled\" + -0.099*\"actually dropped\" + -0.095*\"actually did\" + -0.094*\"aboard freezing\" + -0.092*\"accept snape\" + -0.092*\"aboard compartment\"'),\n",
       " (5,\n",
       "  '0.169*\"abou clamberin\" + 0.135*\"aching harry\" + 0.128*\"aching sweat\" + 0.125*\"actual stealing\" + 0.119*\"added lowered\" + 0.105*\"achieve make\" + 0.098*\"actual wand\" + 0.098*\"actual test\" + 0.097*\"adapted performing\" + 0.095*\"abandoned chintz\"'),\n",
       " (6,\n",
       "  '0.085*\"abergavenny minute\" + 0.085*\"added defiantly\" + -0.083*\"able places\" + -0.081*\"actually attempting\" + 0.077*\"absentmindedly touched\" + 0.076*\"admit formidable\" + 0.075*\"abide laws\" + 0.075*\"accio locket\" + 0.073*\"absentmindedly large\" + 0.073*\"absorbing conversation\"'),\n",
       " (7,\n",
       "  '0.094*\"affronted medieval\" + 0.087*\"accidents like\" + 0.087*\"admission\" + 0.087*\"add task\" + 0.085*\"actress\" + -0.085*\"abou clamberin\" + 0.084*\"acting commentator\" + 0.082*\"abruptly grimmauld\" + 0.082*\"actually managed\" + 0.081*\"accept experience\"'),\n",
       " (8,\n",
       "  '0.099*\"achievable\" + 0.081*\"abruptly sped\" + -0.081*\"address harry\" + -0.080*\"accountant talk\" + 0.080*\"actually met\" + -0.078*\"able start\" + 0.076*\"advertisement home\" + 0.074*\"actually mention\" + 0.071*\"advances new\" + -0.068*\"able delay\"'),\n",
       " (9,\n",
       "  '0.177*\"able places\" + 0.176*\"according mad\" + 0.136*\"absences barty\" + 0.123*\"admitted diadem\" + 0.120*\"aaah\" + 0.119*\"aaaah bletchley\" + 0.116*\"aaaargh\" + 0.116*\"aaaah dumbledore\" + 0.114*\"able play\" + 0.111*\"able pick\"'),\n",
       " (10,\n",
       "  '0.219*\"accepted story\" + 0.155*\"aberforth lost\" + 0.152*\"aberforth order\" + 0.150*\"accidentally plastered\" + 0.144*\"able oh\" + 0.144*\"able change\" + 0.131*\"accepted uncle\" + 0.128*\"able chanced\" + 0.125*\"accepting\" + 0.114*\"aberforth nice\"'),\n",
       " (11,\n",
       "  '0.096*\"admit split\" + 0.094*\"accompany montague\" + 0.094*\"afraid magic\" + 0.079*\"afraid leave\" + 0.078*\"aff\" + -0.076*\"abandoning attempt\" + 0.075*\"accompanied tom\" + -0.072*\"acting alarm\" + -0.071*\"accidents like\" + 0.070*\"afraid fail\"'),\n",
       " (12,\n",
       "  '0.218*\"accepted story\" + 0.162*\"aberforth lost\" + 0.151*\"accidentally plastered\" + 0.145*\"able change\" + 0.131*\"able chanced\" + 0.126*\"aberforth order\" + 0.108*\"acquired recently\" + 0.104*\"aberforth nice\" + 0.103*\"access thoughts\" + 0.101*\"able channel\"'),\n",
       " (13,\n",
       "  '-0.125*\"absentmindedly touched\" + -0.110*\"absolutely spiffing\" + -0.109*\"absolutely wonderful\" + -0.100*\"absolutely sure\" + 0.097*\"able shrieking\" + -0.089*\"accept golpalott\" + -0.085*\"accept help\" + 0.084*\"abandoned chair\" + -0.083*\"accio hagrid\" + -0.083*\"aback\"'),\n",
       " (14,\n",
       "  '-0.078*\"accepted story\" + 0.070*\"achingly familiar\" + -0.069*\"ache wanted\" + 0.069*\"added happier\" + 0.068*\"aaah sat\" + 0.066*\"abound course\" + 0.062*\"absentmindedly touched\" + 0.062*\"accompanying\" + 0.061*\"afraid lord\" + -0.061*\"activity checking\"'),\n",
       " (15,\n",
       "  '-0.109*\"actually attempting\" + -0.106*\"actually think\" + -0.104*\"abruptly breathing\" + -0.096*\"abruptly butterbeer\" + 0.092*\"actually burst\" + -0.091*\"ad different\" + 0.090*\"able worm\" + 0.088*\"ache wanted\" + -0.085*\"abruptly delacours\" + -0.081*\"able timid\"'),\n",
       " (16,\n",
       "  '-0.090*\"accompany minister\" + 0.087*\"add insult\" + -0.087*\"accompany castle\" + 0.080*\"add misery\" + -0.079*\"affronted medieval\" + 0.078*\"accommodated\" + -0.076*\"afraid lot\" + -0.075*\"afraid magic\" + -0.075*\"accompany meandering\" + -0.072*\"afraid flap\"'),\n",
       " (17,\n",
       "  '-0.115*\"ache wanted\" + -0.111*\"ad different\" + -0.108*\"accuse diggory\" + -0.102*\"able worm\" + -0.094*\"actually burst\" + -0.089*\"accountant talk\" + -0.087*\"activity checking\" + 0.086*\"added defiantly\" + 0.083*\"acted thinking\" + -0.083*\"absence usually\"'),\n",
       " (18,\n",
       "  '-0.116*\"add misery\" + 0.106*\"able timid\" + -0.105*\"add list\" + -0.104*\"add insult\" + 0.095*\"able stun\" + -0.091*\"add key\" + 0.090*\"accompanying\" + -0.083*\"add just\" + 0.080*\"abruptly come\" + -0.079*\"add quills\"'),\n",
       " (19,\n",
       "  '-0.148*\"able timid\" + 0.107*\"accounts unusually\" + 0.106*\"advance turned\" + 0.101*\"advanced corner\" + -0.101*\"able stretch\" + -0.099*\"advanced little\" + -0.099*\"abruptly delacours\" + -0.098*\"able stun\" + -0.096*\"abou lily\" + -0.096*\"admittedly fog\"')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_print_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lsi_print_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def convert_LSIdocs2matrix(lsi_docs):\n",
    "    topic_coord_array = []\n",
    "    for doc in lsi_docs:\n",
    "        doc_coords = []\n",
    "        for tup in doc:\n",
    "            doc_coords.append(tup[1])\n",
    "        topic_coord_array.append(doc_coords)\n",
    "    \n",
    "    return np.array(topic_coord_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_coord_array = convert_LSIdocs2matrix(lsi_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.49633013,   0.76659269,  -1.64917045, ...,  -0.10815428,\n",
       "         -2.38524346,   0.62878254],\n",
       "       [  2.49891891,  -0.03404812,  -1.66072778, ...,   0.16852346,\n",
       "         -1.02784256,   1.507728  ],\n",
       "       [  4.1530947 ,   2.29593405,   0.02389489, ...,  -1.68104024,\n",
       "         -1.79290312,   0.08273479],\n",
       "       ..., \n",
       "       [ 11.28442844,  -5.26381999,   0.23202536, ...,   0.133109  ,\n",
       "          0.5382131 ,   0.71849217],\n",
       "       [  9.11452896,   1.95393419,  -1.25900534, ...,   0.61754305,\n",
       "          1.0340638 ,   0.44500571],\n",
       "       [  5.58497671,   2.92222343,   1.43343451, ...,   0.26630126,\n",
       "         -0.05622959,  -0.28190311]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_coord_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2902, 50)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_coord_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster LSI topics in topic coordinate space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runKmeans(data, num_clusters=10):\n",
    "    km = KMeans(n_clusters=num_clusters, init='k-means++', n_init=10)\n",
    "    %time km.fit(data)\n",
    "    cluster_id = km.labels_.tolist()\n",
    "    return cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 438 ms, sys: 14.3 ms, total: 452 ms\n",
      "Wall time: 446 ms\n"
     ]
    }
   ],
   "source": [
    "cluster_id = runKmeans(topic_coord_array, num_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2902, array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_id), np.unique(cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn:\n",
    "Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2902 chunks obtained from all 7 books with a chunk size of 400\n"
     ]
    }
   ],
   "source": [
    "all_documents = []\n",
    "chunk_size = 400\n",
    "for seq in range(1, 8):\n",
    "    book_dict = dtmod.unpickleSomething(BOOK_PATH, \"book{}_chapters.p\".format(seq))\n",
    "    doc = Document(book_dict, chunk_size)\n",
    "    book_documents = doc.docs\n",
    "    all_documents = all_documents + book_documents\n",
    "\n",
    "print(len(all_documents), \"chunks obtained from all 7 books with a chunk size of\", chunk_size )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "def run_sklearnLDA(data_samples):\n",
    "    # count vectorization for LDA\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.75, min_df=2,\n",
    "                                max_features=10000,\n",
    "                                stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(data_samples)\n",
    "    \n",
    "    # Fit LDA model\n",
    "    lda = LatentDirichletAllocation(n_topics=20, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "    lda.fit(tf)\n",
    "    data = lda.transform(tf)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print_top_words(lda, tf_feature_names, n_top_words=20)\n",
    "    \n",
    "    sklearn_lda_components = lda.components_\n",
    "    \n",
    "    return sklearn_lda_components, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "ter got hagrid wand yeh did ollivander yer neville potion mr hogwarts er bit expelled black books look firs right\n",
      "Topic #1:\n",
      "hagrid malfoy just like got knew uncle vernon potter aunt way looking petunia broom letters car crash lily told eyes\n",
      "Topic #2:\n",
      "ron hermione dumbledore did know like looked just snape got professor wand think time looking eyes face right voldemort voice\n",
      "Topic #3:\n",
      "troll fasten baron ending mountain liking knocking quidditch share sinks did desert fastest people steel lumbered playing plates hagrid hermione\n",
      "Topic #4:\n",
      "james evans snivellus snivelly lily hamburger spattering mature ticket exists hagrid know plastic yeh escalator professor dumbledore dursleys petrificus beginning\n",
      "Topic #5:\n",
      "bean saturn uranus decipher dumbfounded cartoon captioned flavor crush mar margins mimed birth zabini mystical timetables consultation dud toadless gringotts\n",
      "Topic #6:\n",
      "team gryffindor wood snitch quidditch angelina katie match broom slytherin field fred quaffle crowd bludger malfoy george firebolt pitch goal\n",
      "Topic #7:\n",
      "leviosa wingardium gar swish flicked baking feather swished basics skyward zoom flick desktop wafting mastered hovered ron flitwick granger hermione\n",
      "Topic #8:\n",
      "kreacher elf master regulus locket mistress elves hallows house peverell cole ignotus hokey gin ownership mudblood destroy peverells despise cupboard\n",
      "Topic #9:\n",
      "albus grindelwald aberforth ariana bathilda kendra gellert flamel nicolas nurmengard prizes card famous shields selfish chocolate frogs frog discovery uses\n",
      "Topic #10:\n",
      "mundungus figg hepzibah burke mrs dennis hokey dudley catastrophe batty merope wisteria airplane whinging neighbour alleyway tobacco tibbles slippers monocle\n",
      "Topic #11:\n",
      "zabini potatoes roast chops blaise pork lamb gravy nicholas boiled ghost peas piled steak beef yorkshire pompously porpington banquet gregory\n",
      "Topic #12:\n",
      "burbage charity dursley drills revolved tut naughty cat cackled traffic town thieves drummed ducking tabby spluttering trophy enraged ickle galloped\n",
      "Topic #13:\n",
      "uncle vernon slughorn dudley aunt petunia scrimgeour dursleys phineas ogden morfin doge dursley nigellus mclaggen marge kitchen magic gaunt boy\n",
      "Topic #14:\n",
      "neville luna filch krum peeves karkaroff fleur xenophilius nick ron corridor staircase headless lovegood madame wall nearly maxime fred students\n",
      "Topic #15:\n",
      "bellatrix greyback narcissa yaxley cattermole lestrange prisoners ted dolohov scabior prophecy fenrir reg snatchers rasped griphook lidded vow andromeda unbreakable\n",
      "Topic #16:\n",
      "hat tournament champion triwizard pod hufflepuff champions tasks sorting stool schools susan compete ravenclaw hall worthy dennis bead seventeen durmstrang\n",
      "Topic #17:\n",
      "hagrid yeh ter mr crouch griphook bagman got winky ollivander forest goblin em hermione dragon rita don did fang fer\n",
      "Topic #18:\n",
      "hagrid mr did dursley goblin dumbledore biscuits mrs professor weighing got wand potter counter letter thirteen know key tell troll\n",
      "Topic #19:\n",
      "cliff frank cottage riddles hangleton merope villagers bryce morfin maid boats marvolo ivy rocks hanged overlooking gossip weakened cook manor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sklearn_lda_components, data = run_sklearnLDA(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2902, 20)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.06004512,   0.0587493 ,   0.06014815, ...,   0.05867293,\n",
       "          0.05947637,   0.05714464],\n",
       "       [  0.05956537,   0.05872073,   0.05907155, ...,   0.05817783,\n",
       "          0.05915199,   0.05905397],\n",
       "       [  0.07544881,   4.49015457,   3.73589282, ...,   6.09488489,\n",
       "         29.11613497,  18.49202694],\n",
       "       ..., \n",
       "       [  2.95413696,   0.07746157,   0.06544147, ...,   0.059033  ,\n",
       "          0.0613584 ,   0.28619427],\n",
       "       [  0.05774296,   0.0582823 ,   0.05796075, ...,   0.06008137,\n",
       "          0.05696976,   0.05853718],\n",
       "       [  0.05874129,   0.05864225,   0.05767886, ...,   0.05722428,\n",
       "          0.05830336,   0.05757705]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_lda_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster sklearn/lda topics in topic coordinate space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runKmeans(data, num_clusters=10):\n",
    "    km = KMeans(n_clusters=num_clusters, init='k-means++', n_init=10)\n",
    "    %time km.fit(data)\n",
    "    cluster_id = km.labels_.tolist()\n",
    "    return cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 171 ms, sys: 8.46 ms, total: 179 ms\n",
      "Wall time: 175 ms\n"
     ]
    }
   ],
   "source": [
    "cluster_id = runKmeans(data, num_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_id.index(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where he would be learning to play that night he bolted his dinner that evening without noticing what he was eating and then rushed upstairs with ron to unwrap the nimbus two thousand at last wow ron sighed as the broomstick rolled onto harry s bedspread even harry who knew nothing about the different brooms thought it looked wonderful sleek and shiny with a mahogany handle it had a long tail of neat straight twigs and nimbus two thousand written in gold near the top as seven of the clock drew nearer harry left the castle and set off in the dusk toward the quidditch field held never been inside the stadium before hundreds of seats were raised in stands around the field so that the spectators were high enough to see what was going on at either end of the field were three golden poles with hoops on the end they reminded harry of the little plastic sticks muggle children blew bubbles through except that they were fifty feet high too eager to fly again to wait for wood harry mounted his broomstick and kicked off from the ground what a feeling he swooped in and out of the goal posts and then sped up and down the field the nimbus two thousand turned wherever he wanted at his lightest touch hey potter come down oliver wood had arrived fie was carrying a large wooden crate under his arm harry landed next to him very nice said wood his eyes glinting i see what mcgonagall meant you really are a natural i am just going to teach you the rules this evening then you will be joining team practice three times a week he opened the crate inside were four different sized balls right said wood now quidditch is easy enough to understand even if it is not too easy to play there are seven players on each side three of them are called chasers three chasers harry repeated as wood took out a bright red ball about the size of a soccer ball this ball s called the quaffle said wood the chasers throw the quaffle to each other and try and get it through one of the hoops to score a goal ten points every time the quaffle goes through one of the hoops follow me the chasers throw the quaffle and put it through the hoops'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
